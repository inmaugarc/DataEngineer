# DATA ENGINEER MASTER
# Data Pipelines with Apache Airflow
A Udacity Data Engineer Nanodegree Project
  
### Table of Contents

1. [Project Motivation](#motivation)
2. [Project Structure](#structure)
3. [Usage](#usage)
4. [Source Datasets](#source_datasets)
5. [Database Model](#database)
6. [File Descriptions](#files)
7. [Dashboards](#dash)
9. [Licensing, Authors, and Acknowledgements](#licensing)
10. [References](#references)


## Project Motivation<a name="motivation"></a> 

There is a startup named Sparkify has grown so much, that they have decided to migrate their data warehouse to a data lake.
The data resides in an S3 bucket, in a directory containing JSON logs about the activity of their users using the app.
There is also a folder with metadata of the songs, also in JSON format.

This startup has asked me, as her data engineer, to build an ELT pipeline with Apache Airflow, that extracts all the data from the S3 buckets, and load them into several facts and dimensional tables in a Redshift Datawarehouse, so that the analytics team can find insights into the songs that our users are listening to.

I will test the database and the ETL pipeline and execute some queries to compare the results with the expected results as a data quality check.

So in this project, I'll apply what I have learned on Apache Airflow and data warehouses to build an ETL pipeline for a data lake hosted on S3. Also I'll need to load data from S3 and process the data into a Redshift with Apache Airflow.

They don't have an easy way to query their data that is stored in a directory of JSON files with the logs on user activity on the app. They have also a directory with JSON metadata on the songs in their app.

The goals of this project are:
> * Create an AWS Redshift datawarehouse
> * Build an ETL pipeline to load data hosted on S3, process the data into facts and dimensional tables on a Redshift datawarehouse using Apache Airflow
> * Run several data quality checks

## DAG </a>
The Airflow DAG will be configurated according to the following guidelines:

The DAG does not have dependencies on past runs
On failure, the task are retried 3 times
Retries happen every 5 minutes
Catchup is turned off
Do not email on retry

## Tasks <a name="Tasks"></a>


> * There are 2 staging tasks to extract information from the log events and songs tables from S3 JSON files
> * After that, we will load the songplays fact table
> * Then load into four dimensional tables: users, songs, artists, time
> * Finally we run several data quality checks to test that everything went correct



## Usage <a name="usage"></a>

$ git clone https://github.com/inmaugarc/DataEngineer/main/p5.git

## Source Datasets <a name="source_datasets"></a>

The files included for the analysis are two datasets:

> * Song Dataset       - This dataset is a subset of the Million Song Dataset () and each file is in JSON format and contains metadata about a song and artist of that song. The files are partitioned by the first three letters of each songs 's track. 
> * Log Dataset        - This dataset consists of log files in JSON format generated by an event simulator based on the songs in the previous dataset. That is to simulate activity logs of users of a music streaming app.

You have to download and unzip the files to test on local

## Database Model <a name="database"></a>

The schema of the Database is a star schema and can be described with the following diagram:
![Alt text](./img/DataLake_Star_schema.png?raw=true "Database_model")

## File Descriptions <a name="files"></a>

These are the python scripts that create the databases schema and all the queries:

1. launch_emr.py: Create an EMR cluster on AWS cloud <br>
2. etl.py: Read the Json files and load that info into the created tables
3. destroy_cluster.py: This script destroys the EMR cluster

## Dashboarding<a name="dash"></a> 

During the project, I have also added an EDA (Exploratory Data Analysis) and I include here some of the graphics for a better understanding of this dataset
<br>

 <br>Users by level
![Alt text](./img/level.png?raw=true "UsersbyLevel")

<br>Level of users' accounts
![Alt text](./img/level_plot.png?raw=true "Level account of Users")

<br>Graphic of Location of Users
![Alt text](./img/location.png?raw=true "Users by Location")

 <br>Histogram of the length users stay
![Alt text](./img/hist.png?raw=true "Histogram")

 <br>Skewness of data
![Alt text](./img/skewness.png?raw=true "Skewness")

## Licensing, Authors, Acknowledgements<a name="licensing"></a>

Must give credit to Udacity for collecting data and because there are some pieces of code taken out from the Data Engineer Nanodegree classrooms. 
Also credits to Udacity Knowledge, where there is important information to develop the project.
And credits to Stackoverflow as it has been a useful source to solve some errors
Some piece of code to plot data has been taken from https://github.com/roshankoirala/pySpark_tutorial

## References <a name="references"></a>
 [The Power of Spark](https://learn.udacity.com/nanodegrees/nd027/parts/cd0030/lessons/ls1965/concepts/626aa254-50bc-4bc7-8fe9-9a4e28527739) <br>
 [Udacity Knowledge](https://knowledge.udacity.com/) <br>
 [StackOverflow](https://stackoverflow.com/) <br>
 [pySparkTutorial](https://github.com/roshankoirala/pySpark_tutorial)<br>
 
 

