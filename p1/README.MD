# DATA ENGINEER MASTER
# Data Modeling with Postgres
A Udacity Data Engineer Nanodegree Project

![Alt text](./img/music_petit.jpg?raw=true "A Project about music!!")

### Table of Contents

1. [Project Motivation](#motivation)
2. [Project Structure](#structure)
3. [Usage](#usage)
4. [Source Datasets](#source_datasets)
5. [Database Model](#database)
6. [File Descriptions](#files)
7. [Licensing, Authors, and Acknowledgements](#licensing)
8. [References](#references)


## Project Motivation<a name="motivation"></a> 

There is a startup named Sparkify that wants to analyze the data they have been collecting about the songs and other user activity on their new music streaming app. They want to answer to the following question:

> * Question 1: What are the songs that users are listening to?


They don't have an easy way to query their data that is stored in a directory of JSON files with the logs on user activity on the app. They have also a directory with JSON metadata on the songs in their app.

The goal of this project are:
> * Creating a Postgress database with tables specifically designed to optimize queries on song play analysis. 
> * Create a database schema, defining fact and dimension tables for a star schema
> * Design and implement the ETL pipeline for the analysis. This pipeline transfers data from files in two directories into some tables in Postgress
> * Use Python and SQL

## Project Structure<a name="structure"></a>

This is the structure of the project:

![Alt text](./img/mytree_img.png?raw=true "tree structure of the project")


## Usage <a name="usage"></a>

The first step is creating the tables for the database and creating a conection to a Postgres database named sparkifydb.
To do so, run the following command in a terminal at the root folder:

> * python create_tables.py


Once the previous command has been executed, we need to start reading the source data and filling the database tables 
with the source data in the right way, that is, we need to start executing an ETL process.For that you can use:

> * python etl.py

## Source Datasets <a name="source_datasets"></a>

The files included for the analysis are two datasets:

> * Song Dataset       - This dataset is a subset of the Million Song Dataset () and each file is in JSON format and contains metadata about a song and artist of that song. The files are partitioned by the first three letters of each songs 's track. 
> * Log Dataset        - This dataset consists of log files in JSON format generated by an event simulator based on the songs in the previous dataset. That is to simulate activity logs of users of a music streaming app.

## Database Model <a name="database"></a>

The schema of the Database is a star schema and can be described with the following diagram:
![Alt text](./img/sparkify_bd.drawio.png?raw=true "Database_model")

## File Descriptions <a name="files"></a>

These are the python scripts that create the databases schema and all the queries:

1. create_tables.py: Prepare all the workspace with a new schema and create all tables <br>
2. etl.py: Read the Json logs and metadata and load that info into the recently created tables
3. sql_queries.py: This script contains all the queries

## Licensing, Authors, Acknowledgements<a name="licensing"></a>

Must give credit to Udacity for collecting data and because there are some pieces of code taken out from the Data Engineer Nanodegree classrooms. 
Also credits to Udacity Knowledge, where there is important information to develop the project
And to Stackoverflow as it has been a useful source to solve some errors

## References <a name="references"></a>
 [Data Modeling with Postgres by Udacity](https://learn.udacity.com/nanodegrees/nd027/parts/cd0029/lessons/ls1961/concepts/1d3c5721-ca17-4483-a84a-a7e999b3d9a3) <br>
 [Udacity Knowledge](https://knowledge.udacity.com/) <br>
 [StackOverflow](https://stackoverflow.com/) <br>
 
