
# DATA ENGINEER MASTER
# Data Lake
A Udacity Data Engineer Nanodegree Project

![Alt text](./img/ryan-quintal-zm42KtKcn9c-unsplash.jpg?raw=true "A Data Lake about music!!")
<br>
Foto de <a href="https://unsplash.com/@ryanquintal?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Ryan Quintal</a> en <a href="https://unsplash.com/es/colecciones/6857718/audiobooks%2C-listening%2C-music?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>
  
### Table of Contents

1. [Project Motivation](#motivation)
2. [Project Structure](#structure)
3. [Usage](#usage)
4. [Source Datasets](#source_datasets)
5. [Database Model](#database)
6. [File Descriptions](#files)
7. [Dashboards](#dash)
9. [Licensing, Authors, and Acknowledgements](#licensing)
10. [References](#references)


## Project Motivation<a name="motivation"></a> 

There is a startup named Sparkify has grown so much, that they have decided to migrate their data warehouse to a data lake.
The data resides in an S3 bucket, in a directory containing JSON logs about the activity of their users using the app.
There is also a folder with metadata of the songs, also in JSON format.

This startup has asked me, as her data engineer, to build an ELT pipeline that extracts all the data from the S3 buckets, procesess them using Spark, and transform that data into dimensional tables, so that the analytics team can find insights into the songs that our users are listening to.

I will test the database and the ETL pipeline and execute some queries to compare the results with the expected results.

So in this project, I'll apply what I have learned on Spark and data lakes to build an ETL pipeline for a data lake hosted on S3. Also I'll need to load data from S3, process the data into analytics tables using Spark, and load them back into S3. Also I will deploy this Spark process on a cluster using AWS.

> * Question 1: What are the songs that users are listening to?
> * Question 2: What are the artists that users are listening to?
> * Question 3: How many users do we have?
> * Question 4: How many users do we have by levels (free/paid)?
> * Question 5: How many users do we have by gender?
> * Question 6: What are the 5 location where users listen the most?

They don't have an easy way to query their data that is stored in a directory of JSON files with the logs on user activity on the app. They have also a directory with JSON metadata on the songs in their app.

The goals of this project are:
> * Create a Data Lake with tables specifically designed to optimize queries on song play analysis. 
> * Build an ETL pipeline to load data hosted on S3, process the data into analytics tables using Spark and load them back into S3
> * Deploy a Spark process on a cluster using AWS


## Project Structure<a name="structure"></a>

This is the structure of the project:
> * dl.cfg: Configuration file with AWS credentials
> * etl.py: Python file with the processes that support the ETL pipeline, that is, reads data from S3 buckets, processess data using Apache Spark, and writes processed data to the output S3 buckets
> * etl.ipynb: Test file for the ETL process
> * launch_emr.py: Script to create an EMR cluster
> * destroy_cluster.py: Script to delete the EMR cluster previously created



## Usage <a name="usage"></a>

The first step is to include the AWS Access Key and Secret key into the dwh.cfg configuration file
Important: Set these keys with creedentials that have read write access to S3!!

The second step is to create the EMR cluster on AWS:
> * python launch_emr.py

The third step is launching the ETL processs 
To do so, run the following command in a terminal at the root folder:

> * python etl.py

Don't forget to delete all the AWS services if you don't want to have an extra cost!!!!

>* python destroy_cluster.py

Here some screenshots of the scripts running:
![Alt text](./img/exec0.png?raw=true "exec0.png")

Here some screenshots of the output files generated
![Alt text](./img/partition.png?raw=true "PartitionedFiles")
![Alt text](./img/tree.png?raw=true "OutputFiles")

<br>

## Source Datasets <a name="source_datasets"></a>

The files included for the analysis are two datasets:

> * Song Dataset       - This dataset is a subset of the Million Song Dataset () and each file is in JSON format and contains metadata about a song and artist of that song. The files are partitioned by the first three letters of each songs 's track. 
> * Log Dataset        - This dataset consists of log files in JSON format generated by an event simulator based on the songs in the previous dataset. That is to simulate activity logs of users of a music streaming app.

You have to download and unzip the files to test on local

## Database Model <a name="database"></a>

The schema of the Database is a star schema and can be described with the following diagram:
![Alt text](./img/DataLake_Star_schema.png?raw=true "Database_model")

## File Descriptions <a name="files"></a>

These are the python scripts that create the databases schema and all the queries:

1. launch_emr.py: Create an EMR cluster on AWS cloud <br>
2. etl.py: Read the Json files and load that info into the created tables
3. destroy_cluster.py: This script destroys the EMR cluster

## Dashboarding<a name="dash"></a> 

During the project, I have also added an EDA (Exploratory Data Analysis) and I include here some of the graphics for a better understanding of this dataset
<br>

 <br>Users by level
![Alt text](./img/level.png?raw=true "UsersbyLevel")

<br>Level of users' accounts
![Alt text](./img/level_plot.png?raw=true "Level account of Users")

<br>Graphic of Location of Users
![Alt text](./img/location.png?raw=true "Users by Location")

 <br>Histogram of the length users stay
![Alt text](./img/hist.png?raw=true "Histogram")

 <br>Skewness of data
![Alt text](./img/skewness.png?raw=true "Skewness")

## Licensing, Authors, Acknowledgements<a name="licensing"></a>

Must give credit to Udacity for collecting data and because there are some pieces of code taken out from the Data Engineer Nanodegree classrooms. 
Also credits to Udacity Knowledge, where there is important information to develop the project.
And credits to Stackoverflow as it has been a useful source to solve some errors
Some piece of code to plot data has been taken from https://github.com/roshankoirala/pySpark_tutorial

## References <a name="references"></a>
 [The Power of Spark](https://learn.udacity.com/nanodegrees/nd027/parts/cd0030/lessons/ls1965/concepts/626aa254-50bc-4bc7-8fe9-9a4e28527739) <br>
 [Udacity Knowledge](https://knowledge.udacity.com/) <br>
 [StackOverflow](https://stackoverflow.com/) <br>
 [pySparkTutorial](https://github.com/roshankoirala/pySpark_tutorial)<br>
 
 
